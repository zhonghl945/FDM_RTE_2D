import os
import numpy as np
import h5py
import numba
from tqdm import tqdm
from perlin_noise import PerlinNoise
import multiprocessing as mp
import time
import random


def scan_hdf5_for_anomalies(file_path, group_name, chunk_size):
    """
    é«˜æ•ˆåœ°æ‰«æHDF5æ–‡ä»¶ä¸­çš„æ•°æ®é›†ï¼ŒæŸ¥æ‰¾NaNå’ŒInfå€¼ã€‚
    """
    if not os.path.exists(file_path):
        print(f"ğŸ›‘ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ '{file_path}'ã€‚è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
        return

    print(f"--- å¼€å§‹æ£€æŸ¥æ–‡ä»¶: {file_path} ---")
    print(f"--- ç›®æ ‡ç»„: '{group_name}' ---")

    try:
        with h5py.File(file_path, 'r') as file:
            if group_name not in file:
                print(f"ğŸ›‘ é”™è¯¯ï¼šåœ¨æ–‡ä»¶ä¸­æ‰¾ä¸åˆ°ç»„ '{group_name}'ã€‚å¯ç”¨ç»„: {list(file.keys())}")
                return

            t_dataset = file[group_name]['T']
            s_dataset = file[group_name]['S']
            num_samples = len(t_dataset)

            print(f"æ€»å…± {num_samples} ä¸ªæ ·æœ¬ï¼Œå¼€å§‹æ‰«æ...")

            # æ£€æŸ¥ 'T' æ•°æ®é›†
            print("\n--- æ­£åœ¨æ‰«æ 'T' æ•°æ®é›† ---")
            for i in tqdm(range(0, num_samples, chunk_size), desc="æ‰«æ T"):
                t_chunk = t_dataset[i: i + chunk_size]

                # æ£€æŸ¥ NaN
                if np.isnan(t_chunk).any():
                    for j in range(len(t_chunk)):
                        if np.isnan(t_chunk[j]).any():
                            sample_idx = i + j
                            nan_count = np.isnan(t_chunk[j]).sum()
                            print(f"\nğŸ›‘ åœ¨ 'T' æ•°æ®é›†çš„ç¬¬ {sample_idx} å·æ ·æœ¬ä¸­å‘ç° {nan_count} ä¸ª NaN å€¼ï¼")
                            print("--- æ£€æŸ¥ç»ˆæ­¢ ---")
                            return

                # æ£€æŸ¥ Inf
                if np.isinf(t_chunk).any():
                    for j in range(len(t_chunk)):
                        if np.isinf(t_chunk[j]).any():
                            sample_idx = i + j
                            inf_count = np.isinf(t_chunk[j]).sum()
                            print(f"\nğŸ›‘ åœ¨ 'T' æ•°æ®é›†çš„ç¬¬ {sample_idx} å·æ ·æœ¬ä¸­å‘ç° {inf_count} ä¸ª Inf å€¼ï¼")
                            print("--- æ£€æŸ¥ç»ˆæ­¢ ---")
                            return

            print("âœ… 'T' æ•°æ®é›†æ£€æŸ¥å®Œæ¯•ï¼Œæœªå‘ç°å¼‚å¸¸ã€‚")

            # æ£€æŸ¥ 'S' æ•°æ®é›†
            print("\n--- æ­£åœ¨æ‰«æ 'S' æ•°æ®é›† ---")
            for i in tqdm(range(0, num_samples, chunk_size), desc="æ‰«æ S"):
                s_chunk = s_dataset[i: i + chunk_size]

                # æ£€æŸ¥ NaN
                if np.isnan(s_chunk).any():
                    for j in range(len(s_chunk)):
                        if np.isnan(s_chunk[j]).any():
                            sample_idx = i + j
                            nan_count = np.isnan(s_chunk[j]).sum()
                            print(f"\nğŸ›‘ åœ¨ 'S' æ•°æ®é›†çš„ç¬¬ {sample_idx} å·æ ·æœ¬ä¸­å‘ç° {nan_count} ä¸ª NaN å€¼ï¼")
                            print("--- æ£€æŸ¥ç»ˆæ­¢ ---")
                            return

                # æ£€æŸ¥ Inf
                if np.isinf(s_chunk).any():
                    for j in range(len(s_chunk)):
                        if np.isinf(s_chunk[j]).any():
                            sample_idx = i + j
                            inf_count = np.isinf(s_chunk[j]).sum()
                            print(f"\nğŸ›‘ åœ¨ 'S' æ•°æ®é›†çš„ç¬¬ {sample_idx} å·æ ·æœ¬ä¸­å‘ç° {inf_count} ä¸ª Inf å€¼ï¼")
                            print("--- æ£€æŸ¥ç»ˆæ­¢ ---")
                            return

            print("âœ… 'S' æ•°æ®é›†æ£€æŸ¥å®Œæ¯•ï¼Œæœªå‘ç°å¼‚å¸¸ã€‚")

    except Exception as e:
        print(f"å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return

    print("\nğŸ‰ æ­å–œï¼å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡ï¼Œæ•°æ®é›†ä¸­æ²¡æœ‰å‘ç°ä»»ä½• NaN æˆ– Inf å€¼ã€‚")


# =============================================================================
# 1. é«˜æ€§èƒ½æ±‚è§£å™¨
# =============================================================================
@numba.jit(nopython=True, fastmath=True)
def _time_loop_numba(T, S, mask_x_in_rect, mask_y_in_rect, results_3d, sample_indices, nt, nx, ny, dt, dx, dy, Cv,
                     T_floor):
    sample_idx_counter = 0
    for n in range(nt):
        Tn = T.copy()
        for j in range(1, ny - 1):
            for i in range(1, nx - 1):
                t_avg_right = (Tn[j, i + 1] + Tn[j, i]) / 2.0
                kappa_right = (t_avg_right ** 3) / 50.0 if mask_x_in_rect[j, i] else (t_avg_right ** 3) / 300.0
                flux_right = kappa_right * (Tn[j, i + 1] - Tn[j, i]) / dx
                t_avg_left = (Tn[j, i] + Tn[j, i - 1]) / 2.0
                kappa_left = (t_avg_left ** 3) / 50.0 if mask_x_in_rect[j, i - 1] else (t_avg_left ** 3) / 300.0
                flux_left = kappa_left * (Tn[j, i] - Tn[j, i - 1]) / dx
                t_avg_top = (Tn[j + 1, i] + Tn[j, i]) / 2.0
                kappa_top = (t_avg_top ** 3) / 50.0 if mask_y_in_rect[j, i] else (t_avg_top ** 3) / 300.0
                flux_top = kappa_top * (Tn[j + 1, i] - Tn[j, i]) / dy
                t_avg_bottom = (Tn[j, i] + Tn[j - 1, i]) / 2.0
                kappa_bottom = (t_avg_bottom ** 3) / 50.0 if mask_y_in_rect[j - 1, i] else (t_avg_bottom ** 3) / 300.0
                flux_bottom = kappa_bottom * (Tn[j, i] - Tn[j - 1, i]) / dy
                divergence = (flux_right - flux_left) / dx + (flux_top - flux_bottom) / dy
                T[j, i] = Tn[j, i] + (dt / Cv) * (divergence + S[j, i])
        for j in range(ny):
            for i in range(nx):
                if T[j, i] < T_floor: T[j, i] = T_floor
        if sample_idx_counter < len(sample_indices) and n == sample_indices[sample_idx_counter]:
            for j in range(ny - 2):
                for i in range(nx - 2):
                    results_3d[sample_idx_counter, j, i] = T[j + 1, i + 1]
            sample_idx_counter += 1
    return T, results_3d


def solve_nonlinear_heat_explicit(nx, ny, nt, S_interior, n_time_samples):
    Lx, Ly, t_final = 1.0, 1.0, 0.5
    Cv, T0, T_floor = 1.0, 0.1, 1e-9
    dx, dy, dt = Lx / (nx - 1), Ly / (ny - 1), t_final / nt
    T = np.full((ny, nx), T0, dtype=np.float64)
    S = np.zeros((ny, nx), dtype=np.float64)
    if S_interior is not None: S[1:-1, 1:-1] = S_interior
    rect_x_range, rect_y_range = (0.25, 0.75), (0.25, 0.75)
    x_coords_full, y_coords_full = np.linspace(0, Lx, nx), np.linspace(0, Ly, ny)
    x_faces_v = x_coords_full[:-1] + dx / 2
    X_faces_v, Y_faces_v = np.meshgrid(x_faces_v, y_coords_full)
    mask_x_in_rect = (X_faces_v >= rect_x_range[0]) & (X_faces_v < rect_x_range[1]) & (Y_faces_v >= rect_y_range[0]) & (
            Y_faces_v < rect_y_range[1])
    y_faces_h = y_coords_full[:-1] + dy / 2
    X_faces_h, Y_faces_h = np.meshgrid(x_coords_full, y_faces_h)
    mask_y_in_rect = (X_faces_h >= rect_x_range[0]) & (X_faces_h < rect_x_range[1]) & (Y_faces_h >= rect_y_range[0]) & (
            Y_faces_h < rect_y_range[1])
    sample_indices = np.linspace(0, nt - 1, num=n_time_samples + 1, dtype=int)[1:]
    results_3d = np.zeros((n_time_samples, ny - 2, nx - 2), dtype=np.float64)
    _, results_3d = _time_loop_numba(T, S, mask_x_in_rect, mask_y_in_rect, results_3d, sample_indices, nt, nx, ny, dt,
                                     dx, dy, Cv, T_floor)
    return results_3d, sample_indices * dt


# =============================================================================
# è®­ç»ƒæºé¡¹ç”Ÿæˆå‡½æ•°åº“
# =============================================================================
def generate_gaussian_base(X, Y, amp_range=(5, 15), center_margin=0.4):
    amp = np.random.uniform(*amp_range)
    cx = np.random.uniform(center_margin, 1 - center_margin)
    cy = np.random.uniform(center_margin, 1 - center_margin)
    return amp * np.exp(-(((X - cx) ** 2 + (Y - cy) ** 2) / 0.2))


def generate_sine_perturbation(X, Y, amp_range=(0, 5), freq_range=(7, 16)):
    amp = np.random.uniform(*amp_range)
    freq_x = np.random.uniform(*freq_range)
    freq_y = np.random.uniform(*freq_range)
    return amp * np.sin(freq_x * np.pi * X) * np.sin(freq_y * np.pi * Y)


def generate_checkerboard_perturbation(X, Y, amp_range=(1, 5), freq_range=(10, 15)):
    amp = np.random.uniform(*amp_range)
    freq = np.random.randint(*freq_range)
    return amp * (2 * ((np.floor(X * freq) + np.floor(Y * freq)) % 2) - 1)


def generate_perlin_perturbation(X, Y, amp_range=(2, 8), octaves_range=(4, 10)):
    amp = np.random.uniform(*amp_range)
    octaves = np.random.randint(*octaves_range)
    seed = np.random.randint(1, 1000)
    noise_gen = PerlinNoise(octaves=octaves, seed=seed)
    nx, ny = X.shape[1], X.shape[0]
    p_noise = np.array([[noise_gen([i / nx, j / ny]) for j in range(ny)] for i in range(nx)]).T
    return amp * p_noise


def generate_diagonal_wave_perturbation(X, Y, amp_range=(2, 5), freq_range=(7, 15)):
    amp = np.random.uniform(*amp_range)
    freq = np.random.uniform(*freq_range)
    direction = np.random.choice([-1, 1])
    return amp * np.sin(freq * np.pi * (X + direction * Y))


def generate_concentric_wave_perturbation(X, Y, amp_range=(1, 3), freq_range=(20, 30), center_margin=0.4):
    amp = np.random.uniform(*amp_range)
    freq = np.random.uniform(*freq_range)
    cx = np.random.uniform(center_margin, 1 - center_margin)
    cy = np.random.uniform(center_margin, 1 - center_margin)
    R = np.sqrt((X - cx) ** 2 + (Y - cy) ** 2)
    return amp * np.sin(freq * R)


def generate_sparse_impulse_perturbation(X, Y, amp_range=(5, 10), sparsity_range=(0.005, 0.05)):
    amp = np.random.uniform(*amp_range)
    sparsity = np.random.uniform(*sparsity_range)
    ny, nx = X.shape
    delta_S = np.zeros_like(X)
    num_points = int(sparsity * nx * ny)
    rows = np.random.randint(0, ny, size=num_points)
    cols = np.random.randint(0, nx, size=num_points)
    values = amp * (2 * np.random.randint(0, 2, size=num_points) - 1)
    delta_S[rows, cols] = values
    return delta_S


# å…¨å±€å˜é‡ï¼Œç”¨äºworkerè¿›ç¨‹åˆå§‹åŒ– ---
# å°†è¿™äº›å˜é‡è®¾ä¸ºå…¨å±€ï¼Œä»¥ä¾¿workerè¿›ç¨‹å¯ä»¥è®¿é—®å®ƒä»¬
# æ³¨æ„ï¼šè¿™ç§æ–¹å¼é€‚ç”¨äºç®€å•çš„å‚æ•°ä¼ é€’ï¼Œæ›´å¤æ‚çš„æƒ…å†µå¯èƒ½éœ€è¦ä¸åŒçš„æ–¹æ³•
g_nx, g_ny, g_nt, g_num_time_snapshots = 0, 0, 0, 0
g_X, g_Y = None, None
g_base_generators, g_perturbation_generators = [], []


def initialize_worker(nx, ny, nt, num_time_snapshots):
    """åˆå§‹åŒ–workerè¿›ç¨‹çš„å…¨å±€å˜é‡"""
    global g_nx, g_ny, g_nt, g_num_time_snapshots, g_X, g_Y
    global g_base_generators, g_perturbation_generators

    g_nx, g_ny, g_nt, g_num_time_snapshots = nx, ny, nt, num_time_snapshots

    Lx, Ly = 1.0, 1.0
    x = np.linspace(0, Lx, nx)
    y = np.linspace(0, Ly, ny)
    g_X, g_Y = np.meshgrid(x, y)

    g_base_generators = [generate_gaussian_base]
    g_perturbation_generators = [
        generate_sine_perturbation,
        generate_checkerboard_perturbation,
        generate_perlin_perturbation,
        generate_diagonal_wave_perturbation,
        generate_concentric_wave_perturbation,
        generate_sparse_impulse_perturbation
    ]
    # é¦–æ¬¡è°ƒç”¨ä»¥ç¼–è¯‘numbaå‡½æ•°
    solve_nonlinear_heat_explicit(2, 2, 1, np.zeros((0, 0)), 1)


def worker_generate_sample(sample_index):
    """
    ã€æ–°å¢ã€‘è¿™æ˜¯å•ä¸ªå·¥ä½œè¿›ç¨‹è¦æ‰§è¡Œçš„ä»»åŠ¡ï¼šç”Ÿæˆä¸€ä¸ª(S, T)æ ·æœ¬ã€‚
    å®ƒä¼šè¿”å›æ ·æœ¬ç´¢å¼•å’Œè®¡ç®—ç»“æœã€‚
    """
    # ä¸ºäº†ä¿è¯æ¯ä¸ªè¿›ç¨‹çš„éšæœºæ€§ä¸åŒï¼Œéœ€è¦é‡æ–°è®¾ç½®éšæœºç§å­
    np.random.seed(os.getpid() + sample_index)

    # ä»å…¨å±€å˜é‡è·å–å‚æ•°
    base_gen = np.random.choice(g_base_generators)
    S_base = base_gen(g_X, g_Y)

    if np.random.rand() > 0.3:
        pert_gen = np.random.choice(g_perturbation_generators)
        delta_S = pert_gen(g_X, g_Y)
    else:
        delta_S = np.zeros_like(g_X)

    S_full = S_base + delta_S
    S_interior = S_full[1:-1, 1:-1]

    T_interior_samples, _ = solve_nonlinear_heat_explicit(
        g_nx, g_ny, g_nt, S_interior, n_time_samples=g_num_time_snapshots
    )

    return sample_index, S_interior.astype('f4'), T_interior_samples.astype('f4')


# =============================================================================
# å¹¶è¡ŒåŒ–æ•°æ®ç”Ÿæˆé€»è¾‘
# =============================================================================
def generate_dataset_parallel(num_samples, file_path, nx, ny, nt, num_time_snapshots, num_workers=4):
    """
    ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œç”Ÿæˆæ•°æ®é›†ã€‚
    """
    dir_name = os.path.dirname(file_path)
    if dir_name: os.makedirs(dir_name, exist_ok=True)

    print(f"å°†åœ¨ {num_workers} ä¸ªCPUæ ¸å¿ƒä¸Šå¹¶è¡Œç”Ÿæˆæ•°æ®...")

    with h5py.File(file_path, 'w') as f:
        print(f"åˆ›å»ºHDF5æ–‡ä»¶: {file_path}")
        group = f.create_group('Train')
        s_dataset = group.create_dataset('S', (num_samples, ny - 2, nx - 2), dtype='f4')
        t_dataset = group.create_dataset('T', (num_samples, num_time_snapshots, ny - 2, nx - 2), dtype='f4')

        # ä½¿ç”¨è¿›ç¨‹æ± 
        # initializerå‡½æ•°ç”¨äºå‘æ¯ä¸ªworkerè¿›ç¨‹ä¼ é€’åªè¯»çš„å…¨å±€å‚æ•°
        with mp.Pool(processes=num_workers, initializer=initialize_worker,
                     initargs=(nx, ny, nt, num_time_snapshots)) as pool:
            # ä½¿ç”¨ imap_unordered æ¥åˆ†å‘ä»»åŠ¡ï¼Œå¹¶ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
            # imap_unordered æ•ˆç‡æ›´é«˜ï¼Œå› ä¸ºå®ƒä¸€æœ‰ç»“æœå°±è¿”å›ï¼Œä¸ä¿è¯é¡ºåº
            results_iterator = pool.imap_unordered(worker_generate_sample, range(num_samples))

            for index, s_result, t_result in tqdm(results_iterator, total=num_samples, desc="æ­£åœ¨ç”Ÿæˆæ•°æ®"):
                # ä¸»è¿›ç¨‹è´Ÿè´£å°†ç»“æœå†™å…¥HDF5æ–‡ä»¶
                s_dataset[index, :, :] = s_result
                t_dataset[index, :, :, :] = t_result

    print(f"\næˆåŠŸå¹¶è¡Œç”Ÿæˆ {num_samples} ç»„æ•°æ®å¹¶ä¿å­˜åˆ° {file_path}")


# =============================================================================
# æµ‹è¯•æ•°æ®ç”Ÿæˆå‡½æ•°
# =============================================================================
def generate_test_dataset(num_test, file_path, nx, ny, nt, num_time_snapshots):
    """
    ç”Ÿæˆå¹¶ä¿å­˜æµ‹è¯•æ•°æ®é›†ã€‚
    è¯¥å‡½æ•°ä¼šæ„é€ ä¸€ä¸ªå›ºå®šçš„æºé¡¹ï¼Œæ±‚è§£åï¼Œå°†ç»“æœå¤åˆ¶num_testæ¬¡å¹¶ä¿å­˜ã€‚
    """
    dir_name = os.path.dirname(file_path)
    if dir_name: os.makedirs(dir_name, exist_ok=True)

    # --- 1. æ„é€ å”¯ä¸€çš„æºé¡¹ S ---
    Lx, Ly = 1.0, 1.0
    x = np.linspace(0, Lx, nx)
    y = np.linspace(0, Ly, ny)
    X, Y = np.meshgrid(x, y)

    # æ³›åŒ–å†…æµ‹è¯•é›†
    # S_full_single = 10 * np.exp(-(((X - 0.5) ** 2 + (Y - 0.5) ** 2) / 0.2))

    # æ³›åŒ–å¤–æµ‹è¯•é›†
    # å®šä¹‰åå­—çš„å‚æ•°
    center_x, center_y = 0.5, 0.5  # ä¸­å¿ƒç‚¹
    arm_thickness = 0.2             # åå­—è‡‚çš„åšåº¦
    arm_length = 0.8                # åå­—è‡‚çš„é•¿åº¦
    amplitude = 10.0                # çƒ­æºå¼ºåº¦ (åœ¨è®­ç»ƒæ•°æ®èŒƒå›´å†…ï¼Œä½†å½¢çŠ¶æ˜¯æ–°çš„)
    # åˆ›å»ºæ°´å¹³è‡‚çš„æ©ç 
    horizontal_mask = (np.abs(X - center_x) < arm_length / 2) & (np.abs(Y - center_y) < arm_thickness / 2)
    # åˆ›å»ºå‚ç›´è‡‚çš„æ©ç 
    vertical_mask = (np.abs(X - center_x) < arm_thickness / 2) & (np.abs(Y - center_y) < arm_length / 2)
    # åˆå¹¶ä¸¤ä¸ªè‡‚æ¥åˆ›å»ºåå­—å½¢
    cross_mask = horizontal_mask | vertical_mask
    # æ ¹æ®æ©ç ç”Ÿæˆæºé¡¹ S
    S_full_single = np.where(cross_mask, amplitude, 0.0)

    S_interior_single = S_full_single[1:-1, 1:-1]

    # --- 2. è°ƒç”¨æ±‚è§£å™¨è·å¾—å¯¹åº”çš„æ¸©åº¦åœº T ---
    print("--> æ­£åœ¨è°ƒç”¨æ±‚è§£å™¨ä¸ºæµ‹è¯•é›†è®¡ç®—æ¸©åº¦åœº (å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
    T_samples_single, _ = solve_nonlinear_heat_explicit(
        nx=nx, ny=ny, nt=nt,
        S_interior=S_interior_single,
        n_time_samples=num_time_snapshots
    )

    # --- 3. å°†Så’ŒTå¤åˆ¶å¤šæ¬¡ ---
    print(f"--> æ­£åœ¨å°†Så’ŒTå¤åˆ¶ {num_test} ä»½...")
    S_replicated = np.repeat(S_interior_single[np.newaxis, :, :], num_test, axis=0)
    T_replicated = np.repeat(T_samples_single[np.newaxis, :, :, :], num_test, axis=0)

    # --- 4. ä¿å­˜åˆ°HDF5æ–‡ä»¶ ---
    with h5py.File(file_path, 'w') as f:
        test_group = f.create_group('Test')
        test_group.create_dataset('S', data=S_replicated.astype('f4'), dtype='f4')
        test_group.create_dataset('T', data=T_replicated.astype('f4'), dtype='f4')

    print(f"\næˆåŠŸç”Ÿæˆ {num_test} ç»„æµ‹è¯•æ•°æ®å¹¶ä¿å­˜åˆ° {file_path}")


# =============================================================================
# 4. ç¨‹åºå…¥å£
# =============================================================================
if __name__ == "__main__":
    # --- ä¸€ä¸ªç¨³å¥çš„é”è·å–å‡½æ•° ---
    def acquire_lock(lock_file_path, timeout=60):
        """
        å°è¯•åœ¨æŒ‡å®šæ—¶é—´å†…è·å–ä¸€ä¸ªæ–‡ä»¶é”ã€‚
        """
        start_time = time.time()
        while time.time() - start_time < timeout:
            try:
                lock_fd = os.open(lock_file_path, os.O_CREAT | os.O_EXCL | os.O_WRONLY)
                return lock_fd
            except (FileExistsError, BlockingIOError):
                wait_time = random.uniform(0.5, 2.0)
                print(f"é”è¢«å ç”¨æˆ–ç³»ç»Ÿç¹å¿™ï¼Œç­‰å¾… {wait_time:.2f} ç§’åé‡è¯•...")
                time.sleep(wait_time)
        return None


    # --- é€šç”¨å‚æ•°è®¾ç½® ---
    NX, NY, NT = 66, 66, 40000
    NUM_TIME_SNAPSHOTS = 16
    np.random.seed(99)

    # å¼ºåˆ¶é™åˆ¶åº•å±‚åº“çš„çº¿ç¨‹æ•°ï¼Œé¿å…å¹¶è¡Œå†²çª
    os.environ['OMP_NUM_THREADS'] = '1'
    os.environ['OPENBLAS_NUM_THREADS'] = '1'
    os.environ['MKL_NUM_THREADS'] = '1'
    os.environ['VECLIB_MAXIMUM_THREADS'] = '1'
    os.environ['NUMEXPR_NUM_THREADS'] = '1'

    # # --- ç”Ÿæˆè®­ç»ƒé›† ---
    # print("\n" + "=" * 50)
    # print("--- å¼€å§‹æ‰§è¡Œï¼šç”Ÿæˆè®­ç»ƒé›† ---")
    # print("=" * 50)
    #
    # NUM_WORKERS = 60
    # NUM_TRAIN_SAMPLES = 50000
    # TRAIN_FILE = './data/2D_Diff_Python.h5'
    # LOCK_FILE_TRAIN = './data/train_generation.lock'
    #
    # os.makedirs(os.path.dirname(TRAIN_FILE), exist_ok=True)
    #
    # lock_fd_train = acquire_lock(LOCK_FILE_TRAIN)
    #
    # if lock_fd_train is not None:
    #     try:
    #         print("æˆåŠŸè·å–è®­ç»ƒé›†æ–‡ä»¶é”ï¼Œå¼€å§‹ç”Ÿæˆ...")
    #         generate_dataset_parallel(
    #             num_samples=NUM_TRAIN_SAMPLES,
    #             file_path=TRAIN_FILE,
    #             nx=NX,
    #             ny=NY,
    #             nt=NT,
    #             num_time_snapshots=NUM_TIME_SNAPSHOTS,
    #             num_workers=NUM_WORKERS
    #         )
    #         print("\n--- è®­ç»ƒé›†ç”Ÿæˆå®Œæ¯• ---")
    #         print("æ­£åœ¨æ‰«æè®­ç»ƒé›†ä»¥ç¡®ä¿æ•°æ®å®Œæ•´æ€§...")
    #         scan_hdf5_for_anomalies(TRAIN_FILE, 'Train', 50)
    #     finally:
    #         print("é‡Šæ”¾è®­ç»ƒé›†æ–‡ä»¶é”...")
    #         os.close(lock_fd_train)
    #         os.remove(LOCK_FILE_TRAIN)
    # else:
    #     print("åœ¨è¶…æ—¶æ—¶é—´å†…æœªèƒ½è·å–è®­ç»ƒé›†é”ï¼Œè¯´æ˜å¦ä¸€ä¸ªè¿›ç¨‹å¯èƒ½æ­£åœ¨é•¿æ—¶é—´è¿è¡Œã€‚è·³è¿‡è®­ç»ƒé›†ç”Ÿæˆã€‚")

    # --- ç”Ÿæˆæµ‹è¯•é›† ---
    print("\n" + "=" * 50)
    print("--- å¼€å§‹æ‰§è¡Œï¼šç”Ÿæˆæµ‹è¯•é›† ---")
    print("=" * 50)

    NUM_TEST_SAMPLES = 50
    TEST_FILE = './data/Test_Data.h5'
    LOCK_FILE_TEST = './data/test_generation.lock'

    os.makedirs(os.path.dirname(TEST_FILE), exist_ok=True)

    lock_fd_test = acquire_lock(LOCK_FILE_TEST)

    if lock_fd_test is not None:
        try:
            print("æˆåŠŸè·å–æµ‹è¯•é›†æ–‡ä»¶é”ï¼Œå¼€å§‹ç”Ÿæˆ...")
            generate_test_dataset(
                num_test=NUM_TEST_SAMPLES,
                file_path=TEST_FILE,
                nx=NX,
                ny=NY,
                nt=NT,
                num_time_snapshots=NUM_TIME_SNAPSHOTS
            )
            print("\n--- æµ‹è¯•é›†ç”Ÿæˆå®Œæ¯• ---")
            print("æ­£åœ¨æ‰«ææµ‹è¯•é›†ä»¥ç¡®ä¿æ•°æ®å®Œæ•´æ€§...")
            scan_hdf5_for_anomalies(TEST_FILE, 'Test', 50)
        finally:
            print("é‡Šæ”¾æµ‹è¯•é›†æ–‡ä»¶é”...")
            os.close(lock_fd_test)
            os.remove(LOCK_FILE_TEST)
    else:
        print("åœ¨è¶…æ—¶æ—¶é—´å†…æœªèƒ½è·å–æµ‹è¯•é›†é”ï¼Œè¯´æ˜å¦ä¸€ä¸ªè¿›ç¨‹å¯èƒ½æ­£åœ¨é•¿æ—¶é—´è¿è¡Œã€‚è·³è¿‡æµ‹è¯•é›†ç”Ÿæˆã€‚")

    print("\næ‰€æœ‰ä»»åŠ¡å·²æ‰§è¡Œå®Œæ¯•ã€‚")
